{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdf78a9-e4e3-45f9-baf6-f998e13f4290",
   "metadata": {},
   "source": [
    "# 2.0 Data Augmentation & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc04db91-de7e-4f00-94db-27fb8e832c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "from helpers.file_system_utils import *\n",
    "from helpers.image_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d6e6a-6b33-4876-b40c-d3fcae6d7e71",
   "metadata": {},
   "source": [
    "## 2.1 Training Dataset\n",
    "\n",
    "### 2.1.1 Image Mirroring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e101ea0-e00e-43f7-87a3-6b90977e68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))\n",
    "\n",
    "def flip_image_horizontally(image_path, save_path):\n",
    "    img = Image.open(image_path)\n",
    "    flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    flipped_img.save(save_path)\n",
    "\n",
    "def pose_image_mirror(root_dir):\n",
    "    error = 0\n",
    "    copy_count = 0\n",
    "    flipped_horizontal_count = 0\n",
    "    flipped_vertical_count = 0\n",
    "\n",
    "    lft_images_dir = root_dir + \"lft/\"\n",
    "    rgt_images_dir = root_dir + \"rgt/\"\n",
    "\n",
    "    for dir_path, suffix in [(lft_images_dir, '-lft.png'), (rgt_images_dir, '-rgt.png')]:\n",
    "        for filename in os.listdir(dir_path):\n",
    "            if is_image_file(filename):\n",
    "                base_filename = os.path.splitext(filename)[0]\n",
    "                source_path = os.path.join(dir_path, filename)\n",
    "                save_path = os.path.join(root_dir, base_filename + suffix)\n",
    "                copyfile(source_path, save_path)\n",
    "                copy_count += 1\n",
    "    print(f\"Successfully copied {copy_count} images\")\n",
    "\n",
    "    for filename in os.listdir(root_dir):\n",
    "        if is_image_file(filename) and filename.lower().endswith('-lft.png'):\n",
    "            base_filename = filename.replace('-lft.png', '')\n",
    "            flipped_path = os.path.join(root_dir, base_filename + '-rgt.png')\n",
    "            source_path = os.path.join(root_dir, filename)\n",
    "            flip_image_horizontally(source_path, flipped_path)\n",
    "            flipped_horizontal_count += 1 \n",
    "        elif is_image_file(filename) and filename.lower().endswith('-rgt.png'):\n",
    "            base_filename = filename.replace('-rgt.png', '')\n",
    "            flipped_path = os.path.join(root_dir, base_filename + '-lft.png')\n",
    "            source_path = os.path.join(root_dir, filename)\n",
    "            flip_image_horizontally(source_path, flipped_path)\n",
    "            flipped_horizontal_count += 1 \n",
    "        else:\n",
    "            error += 1\n",
    "            \n",
    "    print(f\"Successfully flipped horizontally {flipped_horizontal_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d0a45-6d32-4a1a-968f-9be787ba605c",
   "metadata": {},
   "source": [
    "### 2.1.2 Mediapipe Human Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e984fcd5-2ed6-46b6-bcfe-18ac58398cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_landmarker(model_path):\n",
    "    base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "    options = vision.PoseLandmarkerOptions(base_options=base_options, output_segmentation_masks=True)\n",
    "    return vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "def populate_pose_data_with_landmarks(pose_info, landmarks):\n",
    "    for idx, landmark in enumerate(landmarks):\n",
    "        idx_str = str(idx).zfill(2)\n",
    "        pose_info[f'landmark_{idx_str}_x'] = landmark.x\n",
    "        pose_info[f'landmark_{idx_str}_y'] = landmark.y\n",
    "        pose_info[f'landmark_{idx_str}_z'] = landmark.z\n",
    "        pose_info[f'landmark_{idx_str}_v'] = landmark.visibility\n",
    "        \n",
    "def generate_pose_landmark_dictionary(source_dir, model_path, is_video=False):\n",
    "    annotated_dir = create_annotated_directory(source_dir)\n",
    "    filenames = get_image_filenames(source_dir)\n",
    "    landmarker = initialize_landmarker(model_path)\n",
    "\n",
    "    if is_video:\n",
    "        pose_data, errors = batch_process_video_images(annotated_dir, filenames, landmarker)\n",
    "    else:\n",
    "        pose_data, errors = batch_process_static_images(annotated_dir, filenames, landmarker)\n",
    "    \n",
    "    pose_data_df = pd.DataFrame(pose_data)\n",
    "    pose_data_df.to_csv(f'{source_dir}/pose_data_raw.csv', index=False)\n",
    "    \n",
    "    errors = write_error_log(source_dir, errors)\n",
    "    \n",
    "    return print(annotated_dir)\n",
    "\n",
    "def batch_process_video_images(annotated_dir, filenames, landmarker):\n",
    "    pose_data = []\n",
    "    errors = []\n",
    "\n",
    "    for image_file_path in sorted(filenames):\n",
    "        \n",
    "        image_bgr = cv2.imread(image_file_path, cv2.IMREAD_COLOR)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "\n",
    "        image_filename, second, frame_no = parse_video_filename(image_file_path)\n",
    "        detection_result = landmarker.detect(mp_image)\n",
    "\n",
    "        if detection_result.pose_landmarks:\n",
    "            annotate_and_save_image(annotated_dir, image_filename, detection_result, image_rgb, scale=0.4)\n",
    "            \n",
    "            for landmarks in detection_result.pose_landmarks:\n",
    "                pose_info = {\n",
    "                    'image_filename': image_filename,\n",
    "                    'secs': second,\n",
    "                    'frame_no': frame_no\n",
    "                }\n",
    "                populate_pose_data_with_landmarks(pose_info, landmarks)\n",
    "            pose_data.append(pose_info)\n",
    "        else:\n",
    "            errors.append(image_file_path)\n",
    "    \n",
    "    return pose_data, errors\n",
    "\n",
    "def batch_process_static_images(annotated_dir, filenames, landmarker):\n",
    "    \n",
    "    pose_data = []\n",
    "    errors = []\n",
    "\n",
    "    for image_file_path in sorted(filenames):\n",
    "        \n",
    "        image_bgr = cv2.imread(image_file_path, cv2.IMREAD_COLOR)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "\n",
    "        image_filename = os.path.basename(image_file_path)        \n",
    "        pose_name = image_filename.split('.')[0]\n",
    "        \n",
    "        detection_result = landmarker.detect(mp_image)\n",
    "        \n",
    "        if detection_result.pose_landmarks:\n",
    "            annotate_and_save_image(annotated_dir, image_filename, detection_result, image_rgb, scale=1)\n",
    "            \n",
    "            for landmarks in detection_result.pose_landmarks:\n",
    "                pose_info = {\n",
    "                    'image_filename': image_filename,\n",
    "                    'pose_name': pose_name\n",
    "                }\n",
    "                populate_pose_data_with_landmarks(pose_info, landmarks)\n",
    "            pose_data.append(pose_info)\n",
    "        else:\n",
    "            errors.append(image_file_path)\n",
    "    \n",
    "    return pose_data, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d009c-7549-4069-82b7-fc22b6b8f967",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "623abfe1-82f0-4d48-b842-6dcf39a95e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/hpe/pose_landmarker.task'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516d19c-93be-4bde-a9fa-d7d83b8ab770",
   "metadata": {},
   "source": [
    "**Implementation for Benchmark Data (Static Images)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40566352-9c68-4b83-a4a4-bc8621453e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/training/annotated\n"
     ]
    }
   ],
   "source": [
    "dict_source_dir = 'data/training/'\n",
    "generate_pose_landmark_dictionary(source_dir=dict_source_dir,model_path=model_path,is_video=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbebaaec-68d3-45fe-a0e5-3433baab537c",
   "metadata": {},
   "source": [
    "### 2.1.3 Pose Reconstruction using Rotation Matrix\n",
    "\n",
    "See notebook (2-1-3-augmentation-rotation-matrix.ipynb) for details.\n",
    "\n",
    "## 2.2 Test Dataset\n",
    "\n",
    "### 2.2.1 Decompose Video to Image Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9213f38-2e8a-4613-ab4c-986693649fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.video_utils import *\n",
    "\n",
    "def decompose_video_to_frames(input_video_path, output_dir):\n",
    "    if not is_video_openable(input_video_path):\n",
    "        return False\n",
    "    fps, _, _ = get_video_properties(input_video_path)\n",
    "    image_count = process_video_images(input_video_path, output_dir, fps)\n",
    "    print(f\"Processed {image_count} frames from the video.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5fce7c-b7c0-4744-8bec-5f6cc635d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 247 frames from the video.\n",
      "Processed 133 frames from the video.\n",
      "Processed 96 frames from the video.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_video_path = 'data/test/superman/01.mp4'\n",
    "output_dir = 'data/test/processed/superman/01/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/superman/02.mp4'\n",
    "output_dir = 'data/test/processed/superman/02/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/superman/03.mp4'\n",
    "output_dir = 'data/test/processed/superman/03/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c34a4a0-c29a-480a-bb5f-cf2213980c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 247 frames from the video.\n",
      "Processed 133 frames from the video.\n",
      "Processed 96 frames from the video.\n",
      "Processed 849 frames from the video.\n",
      "Processed 342 frames from the video.\n",
      "Processed 143 frames from the video.\n",
      "Processed 96 frames from the video.\n",
      "Processed 185 frames from the video.\n",
      "Processed 123 frames from the video.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_video_path = 'data/test/raw_videos/superman/01.mp4'\n",
    "output_dir = 'data/test/processed/superman/01/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/superman/02.mp4'\n",
    "output_dir = 'data/test/processed/superman/02/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/superman/03.mp4'\n",
    "output_dir = 'data/test/processed/superman/03/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/crucifix/01.mp4'\n",
    "output_dir = 'data/test/processed/crucifix/01/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/crucifix/02.mp4'\n",
    "output_dir = 'data/test/processed/crucifix/02/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/crucifix/03.mp4'\n",
    "output_dir = 'data/test/processed/crucifix/03/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/fireman/01.mp4'\n",
    "output_dir = 'data/test/processed/fireman/01/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/fireman/02.mp4'\n",
    "output_dir = 'data/test/processed/fireman/02/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n",
    "\n",
    "input_video_path = 'data/test/raw_videos/fireman/03.mp4'\n",
    "output_dir = 'data/test/processed/fireman/03/'\n",
    "decompose_video_to_frames(input_video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc21c0-032f-4795-b924-df1bd12b32b3",
   "metadata": {},
   "source": [
    "### 2.2.2 Mediapipe Human Pose Estimation (for Videos)\n",
    "\n",
    "Body landmark features are extracted the same way as the training data, but descriptive labels would be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e96579e4-8aec-4647-a908-d50de3a58b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test/processed/superman/01/annotated\n",
      "data/test/processed/superman/02/annotated\n",
      "data/test/processed/superman/03/annotated\n",
      "data/test/processed/crucifix/01/annotated\n",
      "data/test/processed/crucifix/02/annotated\n",
      "data/test/processed/crucifix/03/annotated\n",
      "data/test/processed/fireman/01/annotated\n",
      "data/test/processed/fireman/02/annotated\n",
      "data/test/processed/fireman/03/annotated\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'data/test/processed/superman/01/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/superman/02/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/superman/03/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/crucifix/01/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/crucifix/02/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/crucifix/03/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/fireman/01/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/fireman/02/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)\n",
    "\n",
    "output_dir = 'data/test/processed/fireman/03/'\n",
    "generate_pose_landmark_dictionary(source_dir=output_dir,model_path=model_path,is_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715918a-209a-45fe-befe-cb0acd8977fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
